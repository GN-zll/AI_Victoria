{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ad99a-fb7a-402f-8b91-7b2c5fd71535",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d8611-a40b-49e7-8b26-4990fb32df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import accelerate\n",
    "import jiwer\n",
    "import pandas as pd\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1d0ad-6904-48c2-a1fd-e8a29dc092bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "gt = pd.read_csv('/home/jupyter/datasphere/project/rodion_dir/GT_train.csv', sep = ';')\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f5106-f672-406a-ace8-8f91a37c9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_audio_files(directory):\n",
    "    audio_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".mp3\", \".m4a\", \".ogg\", \".flac\", \".aac\", \".wav\")):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Независимо от формата файла, создаем путь с .wav\n",
    "                wav_path = file_path.rsplit(\".\", 1)[0] + \".wav\"\n",
    "                \n",
    "                # Преобразование файла в формат wav, одноканальный и 16 кГц\n",
    "                audio = AudioSegment.from_file(file_path)\n",
    "                audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "                \n",
    "                # Экспортируем в wav\n",
    "                audio.export(wav_path, format=\"wav\")\n",
    "                \n",
    "                audio_files.append(wav_path)  # Сохранение пути к новому файлу\n",
    "    return audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3c674-07ff-4f98-b02d-badc1eb6a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_directory(pipe, directory: str) -> pd.DataFrame:\n",
    "    audio_files = find_audio_files(directory)\n",
    "    transcriptions = []\n",
    "    names = []\n",
    "    for file_path in audio_files:\n",
    "        transcription = pipe(sample, generate_kwargs={\"language\": \"ru\", \"task\": \"transcribe\"})[\"text\"]\n",
    "        transcriptions.append(transcription)\n",
    "        names.append(file_path[14:-17])\n",
    "    submission = pd.DataFrame({\"Наименование аудиозаписи\": names, \"predicted\": transcriptions})\n",
    "    return submission\n",
    "\n",
    "def calc_sub_metric(submission, gt=gt) -> pd.DataFrame:\n",
    "    df = gt.merge(submission, how = \"left\", on = 'Наименование аудиозаписи')\n",
    "    print(df)\n",
    "    df['WER'] = [1-jiwer.wer(x[0],x[1]) for x in zip(df['Транскрибированный текст'], df['predicted'])]\n",
    "    return df\n",
    "\n",
    "\n",
    "def test_model(pipe, directory, gt=gt) ->pd.DataFrame:\n",
    "    submission = predict_directory(pipe, directory)\n",
    "    return calc_sub_metric(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66da3fc-94b9-43f8-abdc-98229e37d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/jupyter/datasphere/project/rodion_dir/train\"\n",
    "audio_files = make_audio_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a4b77-9ee3-4311-b7f2-0ab2efc4a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937faf4a-449f-414a-bc68-bc104261dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = audio_files[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af478f-c902-4b37-9929-ce500a1f3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(sample, generate_kwargs={\"language\": \"ru\", \"task\": \"transcribe\"})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8662c76-cc7f-4047-98ca-2622974196b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd534c25-e643-4e88-9461-7fa11da84cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
